{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4f8f1115-66e6-44e5-a062-0d034fcfc631",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import mediapipe as mp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b36d8164-a7d6-487b-954a-cb2922b3cba6",
   "metadata": {},
   "source": [
    "**MediaPipe, a framework by Google that provides machine learning solutions for real-time computer vision tasks.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01846311-06c5-4526-bb3f-b9b4a601f888",
   "metadata": {},
   "source": [
    "# DETECTING FACE IN AN IMAGE:\n",
    "**Steps:** \n",
    "\n",
    "**1) Read Image**\n",
    "\n",
    "**2) Detect Face**\n",
    "\n",
    "**3) Blur Face**\n",
    "\n",
    "**4) Save Image**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae11fd61-18e5-46c8-968c-a7ae34a573ea",
   "metadata": {},
   "source": [
    "## 1) Read Image:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "00be00c8-0c0e-4026-baab-da49b44047cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "img = cv2.imread('Face Image.jpg')\n",
    "cv2.imshow('Image', img)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5b600fc-b698-4fb9-b525-4c67c80496dc",
   "metadata": {},
   "source": [
    "## 2) Detect Face:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2f5a0bab-6c93-46a8-83cb-6297c598ffb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "H, W, Channel = img.shape      # Getting image dimention\n",
    "\n",
    "face_detector = mp.solutions.face_detection\n",
    "# mp.solutions.face_detection is a module in MediaPipe, that specifically deals with face detection in images or videos. It provides a Bounding Box\n",
    "# Coordinates. It returns a live feed from the webcam with rectangles drawn around detected faces. By assigning it to face_detector, we create a shorter \n",
    "# alias, making it easier to call later in the code.\n",
    "\n",
    "with face_detector.FaceDetection(model_selection = 0, min_detection_confidence = 0.5) as face_detection:    # 1️⃣ Creating face detection object\n",
    "    \n",
    "    rgb_img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)                     # 2️⃣ Converting image to RGB\n",
    "    \n",
    "    result = face_detection.process(rgb_img)                           # 3️⃣ It detects all faces in the image, & stores them in result object\n",
    "\n",
    "    if result.detections:\n",
    "        for detection in result.detections:                                # 4️⃣ Looping Through All Detected Faces\n",
    "        \n",
    "            location_data = detection.location_data                        # Gives bounding box details\n",
    "        \n",
    "            box = location_data.relative_bounding_box                      # Provides the relative coordinates of the bounding box\n",
    "\n",
    "            x1, y1, w, h = box.xmin, box.ymin, box.width, box.height       # Getting relative coordinates\n",
    "\n",
    "            # Converting those relative coordinates to absolute pixel values\n",
    "            x1 = int(x1*W)                                                 # Converting from relative to absolute values ensures the rectangle is \n",
    "            y1 = int(y1*H)                                                 # drawn at the correct location on the image.\n",
    "            w = int(w*W)\n",
    "            h = int(h*H)\n",
    "\n",
    "            img = cv2.rectangle(img, (x1, y1), (x1+w, y1+h), (0, 255, 0), 2)  # 6️⃣ Drawing bounding boxes on the detected face\n",
    "\n",
    "    cv2.imshow('Face in Image', img)\n",
    "    cv2.waitKey(0)\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e3e20e1-0cec-41ce-8e8a-12bff0920f53",
   "metadata": {},
   "source": [
    "1️⃣ Creating a Face Detection Object:\n",
    "\n",
    "**`with face_detector.FaceDetection(model_selection=0, min_detection_confidence=0.5) as face_detection:`**  \n",
    "\n",
    "- `face_detector.FaceDetection(...)` initializes the **face detection model**.  \n",
    "- **`model_selection=0`**:  \n",
    "  - `0`: Uses a **short-range model** optimized for **faces closer to the camera**.  \n",
    "  - `1`: Uses a **full-range model** for **detecting small or distant faces**.  \n",
    "- **`min_detection_confidence=0.5`**:  \n",
    "  - Sets the **minimum confidence threshold** (50%) for detecting a face. Here it ensures detections have at least 50% confidence. \n",
    "- The **`with` statement** ensures that **resources are properly released** after use.  \n",
    "\n",
    "✅ **Purpose**: Initializes the **face detection model** with a specified confidence level.  \n",
    "\n",
    "\n",
    "2️⃣ Converting Image to RGB:\n",
    "**rgb_img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)** converts the image from BGR to RGB format.\n",
    "OpenCV loads images in BGR, but MediaPipe requires RGB. Hence, it Converts the image to RGB so that MediaPipe can process it.\n",
    "\n",
    "3️⃣ Processing the Image for Face Detection:\n",
    "\n",
    "**result = face_detection.process(rgb_img)**, here face_detection.process(rgb_img) runs the face detection model on the image. **face_detection.process(rgb_img)** analyzes the image and stores the detected face(s) in the **result.detections** list (if any are found), the list will be None if no faces are detected. Here it runs face detection on the converted RGB image, and then stores the detected faces in the result object.\n",
    "```python\n",
    "If no faces are detected, result.detections will be None, so handling this prevents runtime errors.\n",
    "if result.detections:\n",
    "    for detection in result.detections:\n",
    "```\n",
    "\n",
    "4️⃣ Looping Through All Detected Faces:\n",
    "\n",
    "**for detection in result.detections:**, here result.detections contains all detected faces. And the loop iterates over each detected face. This ensures that multiple faces can be processed if present.\n",
    "\n",
    "5️⃣ Extracting Face Bounding Box Coordinates: Retrieves bounding box information for each detected face.\n",
    "\n",
    "location_data = detection.location_data: **detection.location_data** contains the bounding box details. This stores details about the detected face, including: Bounding box information, Key landmarks (eyes, nose, mouth, etc.), Face rotation (if available).\n",
    "\n",
    "box = location_data.relative_bounding_box: **.relative_bounding_box** provides the face bounding box in relative coordinates (0 to 1). It contains: xmin: X-coordinate of the top-left corner, ymin: Y-coordinate of the top-left corner, width: Width of the bounding box, and height: Height of the bounding box.\n",
    "\n",
    "**Relative coordinates** express positions as fractions of the total width and height of an image, rather than fixed pixel values. The values range from 0 to 1, where: 0 represents the starting edge of the image, and 1 represents the ending edge of the image.**Example:** If a face is detected in an image of size (1000 × 800) pixels, the face detector might return:\n",
    "| **Relative Coordinates**       | **Description**          | **Absolute Pixel Values (W = 1000, H = 800)** |\n",
    "|--------------------------------|--------------------------|-----------------------------------------------|\n",
    "| `box.xmin = 0.3`               | 30% from the left        | `x1 = int(0.3 * 1000) = 300` pixels          |\n",
    "| `box.ymin = 0.2`               | 20% from the top         | `y1 = int(0.2 * 800) = 160` pixels           |\n",
    "| `box.width = 0.4`              | 40% of the total width   | `w = int(0.4 * 1000) = 400` pixels           |\n",
    "| `box.height = 0.5`             | 50% of the total height  | `h = int(0.5 * 800) = 400` pixels            |\n",
    "\n",
    "6️⃣ Drawing Bounding Boxes on Detected Faces:\n",
    " \n",
    "img = cv2.rectangle(img, (x1, y1), (x1 + w, y1 + h), (0, 255, 0), 2) Draws a green rectangle around the detected face.\n",
    "```python\n",
    "(x1, y1): Top-left corner of the bounding box.\n",
    "(x1 + w, y1 + h): Bottom-right corner.\n",
    "(0, 255, 0) represents the color green in OpenCV’s BGR format (Blue, Green, Red).\n",
    "2: Thickness of the rectangle.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b6f9524-2ed1-43f2-b412-c36562a19ad1",
   "metadata": {},
   "source": [
    "## 3) Blur Image:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ec8fb652-7ef0-4722-9e64-4609186e3e9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "with face_detector.FaceDetection(model_selection = 0, min_detection_confidence = 0.5) as face_detection:    # Creating face detection object\n",
    "    \n",
    "    rgb_img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)                   \n",
    "    \n",
    "    result = face_detection.process(rgb_img)                           # It detects all faces in the image, & stores them in result object\n",
    "\n",
    "    if result.detections:\n",
    "        for detection in result.detections:                                # Looping Through All Detected Faces\n",
    "        \n",
    "            location_data = detection.location_data                        # Gives bounding box details\n",
    "        \n",
    "            box = location_data.relative_bounding_box                      # Provides the relative coordinates of the bounding box\n",
    "\n",
    "            x1, y1, w, h = box.xmin, box.ymin, box.width, box.height       # Getting relative coordinates\n",
    "\n",
    "            x1 = int(x1*W)                                                 \n",
    "            y1 = int(y1*H)                                                 \n",
    "            w = int(w*W)\n",
    "            h = int(h*H)\n",
    "\n",
    "            # Blur Image\n",
    "            img[y1:y1 + h, x1:x1 + w, :] = cv2.blur(img[y1:y1 + h, x1:x1 + w, :], (30, 30))\n",
    "\n",
    "    cv2.imshow('Face in Image', img)\n",
    "    cv2.waitKey(0)\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac1681a3-2282-4db5-a696-211a5c978790",
   "metadata": {},
   "source": [
    "**img[y1:y1 + h, x1:x1 + w, :] = cv2.blur(img[y1:y1 + h, x1:x1 + w, :], (30, 30)):** This line applies a blur effect to a specific region of the image (the detected face). Let's break it down:\n",
    "\n",
    "1️⃣ Extracting the Face Region:\n",
    "```python\n",
    "img[y1:y1 + h, x1:x1 + w, :]\n",
    "y1:y1 + h → Selects rows from y1 to y1 + h (height of the bounding box).\n",
    "x1:x1 + w → Selects columns from x1 to x1 + w (width of the bounding box).\n",
    ": → Selects all color channels (R, G, B).\n",
    "This extracts the detected face region from the image.\n",
    "```\n",
    "2️⃣ Applying Blur Using cv2.blur():\n",
    "```python\n",
    "cv2.blur(img[y1:y1 + h, x1:x1 + w, :], (30, 30))\n",
    "cv2.blur() → Averages pixel values within a given kernel size to create a blur effect.\n",
    "(30, 30) → Defines the kernel size (30×30 pixels), meaning each pixel''s value is replaced with the average of its surrounding 30×30 pixels.\n",
    "This results in a blurred face. A larger kernel means a stronger blur.\n",
    "```\n",
    "3️⃣ Assigning the Blurred Region Back:\n",
    "```python\n",
    "img[y1:y1 + h, x1:x1 + w, :] = ...\n",
    "This replaces the original face region with its blurred version in the img array, this blurs only the detected face while keeping the rest of the image unchanged.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2381d0dc-354e-4319-b380-67956f714482",
   "metadata": {},
   "source": [
    "## 4) Save Image:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "47b6f399-8f71-4899-be52-0f0d1fac646c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv2.imwrite('Blur Face Image.jpg', img)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6507321b-8bf4-4bd5-afd6-f89849dd7a2d",
   "metadata": {},
   "source": [
    "# ANONYMIZING FACES THROUGH WEBCAM:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "55e1995a-8010-42fa-befc-9166991f6fc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definig a function to do all the processing of image/Frame, we will give it the image/frame and face_detection object, and it will return us the\n",
    "# blurred image\n",
    "\n",
    "def img_processing(img, face_detection):\n",
    "    rgb_img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)                   \n",
    "    \n",
    "    result = face_detection.process(rgb_img)                               # It detects all faces in the image, & stores them in result object\n",
    "\n",
    "    if result.detections:\n",
    "        for detection in result.detections:                                # Looping Through All Detected Faces\n",
    "        \n",
    "            location_data = detection.location_data                        # Gives bounding box details\n",
    "        \n",
    "            box = location_data.relative_bounding_box                      # Provides the relative coordinates of the bounding box\n",
    "\n",
    "            x1, y1, w, h = box.xmin, box.ymin, box.width, box.height       # Getting relative coordinates\n",
    "\n",
    "            x1 = int(x1*W)                                                 \n",
    "            y1 = int(y1*H)                                                 \n",
    "            w = int(w*W)\n",
    "            h = int(h*H)\n",
    "\n",
    "            # Ensure coordinates are within valid range\n",
    "            x1, y1 = max(0, x1), max(0, y1)\n",
    "            w, h = min(W - x1, w), min(H - y1, h)\n",
    "\n",
    "            # Ensure width and height are positive\n",
    "            if w > 0 and h > 0:\n",
    "                \n",
    "                img[y1:y1 + h, x1:x1 + w, :] = cv2.blur(img[y1:y1 + h, x1:x1 + w, :], (30, 30))  # Blur Image\n",
    "\n",
    "\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "10047663-b190-4c8f-b12e-e327c6551258",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Face Detection model from Mediapipe\n",
    "mp_face_detection = mp.solutions.face_detection\n",
    "face_detection = mp_face_detection.FaceDetection(min_detection_confidence=0.5)\n",
    "\n",
    "\n",
    "cam_vid = cv2.VideoCapture(0)                                      # Start Video Capture0 means default webcam\n",
    "\n",
    "\n",
    "while True:\n",
    "    ret, frame = cam_vid.read()                                    # Read a frame from the webcam\n",
    "    if ret == False:\n",
    "        break                                                      # Exit if the frame is not captured properly\n",
    "    \n",
    "    H, W, _ = frame.shape                                          # Get frame dimensions\n",
    "\n",
    "    processed_frame = img_processing(frame, face_detection)        # Process the frame using the function\n",
    "\n",
    "    cv2.imshow(\"Blurred Face\", processed_frame)                    # Show the output frame\n",
    "\n",
    "    if cv2.waitKey(40) & 0xFF == ord('q'):                         # Exit on pressing 'q'\n",
    "        break\n",
    "\n",
    "\n",
    "\n",
    "cam_vid.release()                                                   # Release resources like webcam\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68c2b8fd-b340-46bd-82b4-b5dea1ff8684",
   "metadata": {},
   "source": [
    "# BLURRING FACE IN A VIDEO:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e99d436f-85e2-4395-bbbd-219dc88fd1e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Amount of time taken by one frame is 40.0 milliseconds.\n"
     ]
    }
   ],
   "source": [
    "vid = cv2.VideoCapture('Human Video.mp4')\n",
    "\n",
    "# This is to calculate the time taken by 1 frame in our video, which equals 1/FPS (Frames Per Second):\n",
    "\n",
    "fps = vid.get(cv2.CAP_PROP_FPS)             # Get frames per second\n",
    "frame_time = 1 / fps                        # Time per frame in seconds\n",
    "print('Amount of time taken by one frame is', frame_time*1000, 'milliseconds.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1fb46039-b61e-48b9-82f0-9d640c57efec",
   "metadata": {},
   "outputs": [],
   "source": [
    "while True:\n",
    "    ret, frame = vid.read()                                    # Read a frame from the webcam\n",
    "    if ret == False:\n",
    "        break                                                      # Exit if the frame is not captured properly\n",
    "    \n",
    "    H, W, _ = frame.shape                                          # Get frame dimensions\n",
    "\n",
    "    processed_frame = img_processing(frame, face_detection)        # Process the frame using the function\n",
    "\n",
    "    cv2.imshow(\"Blurred Face\", processed_frame)                    # Show the output frame\n",
    "\n",
    "    if cv2.waitKey(40) & 0xFF == ord('q'):                         # Exit on pressing 'q'\n",
    "        break\n",
    "\n",
    "\n",
    "\n",
    "vid.release()                                                   # Release resources like webcam\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f61d3ba6-0b23-4f35-af59-84621704a1b9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
